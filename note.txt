step 0 create namespace for all the deployments 
step 1 deploy strimzi application that simplifies the process to mointor the kafka clusters
        each operator manages kafka and performs a seperate function

        using helm install the repo 
        helm repo add strimzi https://strimzi.io/charts

        then install using 
        helm install my-strimzi-kafka-operator strimzi/strimzi-kafka-operator --version 0.38.0 -n kafka
    
    it contains

- Cluster Operator  for kafka clusters
The Cluster Operator handles the deployment and management of Apache Kafka clusters on Kubernetes. It automates the setup of Kafka brokers, and other Kafka components and resources.
including dependies like zookeeper

- Topic Operator   for topics
The Topic Operator manages the creation, configuration, and deletion of topics within Kafka clusters.

- User Operator  for kafka users
The User Operator manages Kafka users that require access to Kafka brokers.

When you deploy Strimzi, you first deploy the Cluster Operator. The Cluster Operator is then ready to handle the deployment of Kafka. You can also deploy the Topic Operator and User Operator using the Cluster Operator (recommended) or as standalone operators. You would use a standalone operator with a Kafka cluster that is not managed by the Cluster Operator.


step 2 deploy apache kafka 

- the components deployed in kafka
- a broker uses apache ZooKeeper for storing configurations data and for cluster coordination

- A broker is referred to a server or node that orchestrates the storage and passing of messages 

- ZooKeeper cluster of replicated ZooKeeper instances

- topic = provides a destination for the storage of data each topic is split into one or more partitions

- kafka cluster a group of broker instances

- partition - partitioning takes a single topic log and breaks it into multiple logs each of whiich can live on a seperate node in a kafka cluster
- Kafka Connect cluster for external data connections

- Kafka MirrorMaker cluster to mirror the Kafka cluster in a secondary cluster

Kafka MirrorMaker replicates data between two Kafka clusters, within or across data centers.

MirrorMaker takes messages from a source Kafka cluster and writes them to a target Kafka cluster.

- Kafka Exporter to extract additional Kafka metrics data for monitoring

Kafka Exporter

Kafka Exporter extracts data for analysis as Prometheus metrics, primarily data relating to offsets, consumer groups, consumer lag and topics. Consumer lag is the delay between the last message written to a partition and the message currently being picked up from that partition by a consumer

- Kafka Bridge to make HTTP-based requests to the Kafka cluster

Cruise Control to rebalance topic partitions across broker nodes

- use cases of kafka cases 

- and kafka uses 


once you have deployed your kafka cluster 

then you deploy the my sql cluster database


to connect the sql database serving as a microservice 

you need to deploy a kafka cluster connect

Kafka Connect

Kafka Connect is an integration toolkit for streaming data between Kafka brokers and other systems using Connector plugins. Kafka Connect provides a framework for integrating Kafka with an external data source or target, such as a database like my-sql maria-db, for import or export of data using connectors like debzium.
Connectors are plugins that provide the connection configuration needed.
like debzium connector

A source connector pushes external data into Kafka.

A sink connector extracts data out of Kafka

External data is translated and transformed into the appropriate format.

You can deploy Kafka Connect with build configuration that automatically builds a container image with the connector plugins you require for your data connections.

then deploy a debezium connector  to the cluster connector


after deploying the mysql container for collection of the data 

then you use docker image to create your kafka connector 

first download the extract the debezium mysql connector archive 

curl https://repo1.maven.org/maven2/io/debezium/debezium-connector-mysql/1.0.0.Final/debezium-connector-mysql-1.0.0.Final-plugin.tar.gz \
| tar xvz


the prepare a Dockerfile which adds the connector files to the strimzi kafka connect image

Docker build . -t henriksin1/connect-debezium

docker push henriksin1/connect-debezium


































































### Step 1: Encode PostgreSQL admin username and password using base64
    Linux
        # Encode a string to base64
            echo  'admin' | base64
       
        # Decode our base64 string
            echo 'YQBkAG0AaQBuAA==' | base64 --decode

    Windows PowerShell
        # Encode a string
            $MYTEXT = 'admin'
            $ENCODED = [Convert]::ToBase64String([Text.Encoding]::Unicode.GetBytes($MYTEXT))
            Write-Output $ENCODED

        # Decode a string
            $MYTEXT = 'YQBkAG0AaQBuAA=='
            $DECODED = [System.Text.Encoding]::Unicode.GetString([System.Convert]::FromBase64String($MYTEXT))
            Write-Output $DECODED

    Python
        # Encoding a string
            import base64
            encoded = base64.b64encode(b'admin')
            encoded

        # Decoding
            decoded = base64.b64decode(b'dGhlZGV2b3BzbGlmZS5jb20=')
            decoded